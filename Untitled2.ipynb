{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f920ab32",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5bf11",
   "metadata": {},
   "source": [
    "Let’s walk through the process of specifying a **theoretical Simple Linear Model** (SLM) using an arbitrary choice of values for its components: \n",
    "\n",
    "- Predictor variables \\( x_i \\)\n",
    "- Slope and intercept parameters \\( \\beta_0 \\) and \\( \\beta_1 \\)\n",
    "- Normally distributed errors \\( \\epsilon_i \\) with standard deviation \\( \\sigma \\)\n",
    "\n",
    "The model can be described mathematically as:\n",
    "\n",
    "\\[\n",
    "Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n",
    "\\]\n",
    "\n",
    "Here’s the breakdown:\n",
    "1. **Define the Predictor \\( x_i \\)**: These values can be chosen arbitrarily and don’t depend on any particular distribution. For simplicity, we’ll generate them from a uniform distribution.\n",
    "2. **Specify Model Parameters \\( \\beta_0 \\) and \\( \\beta_1 \\)**: These are chosen a priori (before the data is generated) to define the line of the model.\n",
    "3. **Generate Error Terms \\( \\epsilon_i \\)**: These errors are drawn from a normal distribution with mean 0 and standard deviation \\( \\sigma \\), adding variability around the line defined by \\( \\beta_0 \\) and \\( \\beta_1 \\).\n",
    "4. **Calculate \\( Y_i \\)**: Using the formula \\( Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\), we obtain the outcome values.\n",
    "\n",
    "Finally, we can visualize the \"true\" line and the generated \\( Y \\) points with some scatter around it using **Plotly**.\n",
    "\n",
    "### Python Code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import uniform, norm\n",
    "\n",
    "# Set parameters for the linear model\n",
    "n = 50              # Number of data points\n",
    "beta_0 = 3          # Intercept\n",
    "beta_1 = 1.5        # Slope\n",
    "sigma = 1           # Standard deviation of the error term\n",
    "\n",
    "# Generate predictor values (X) uniformly between 0 and 10\n",
    "np.random.seed(0)   # For reproducibility\n",
    "x = uniform.rvs(0, 10, size=n)\n",
    "\n",
    "# Generate normally distributed errors with mean 0 and standard deviation sigma\n",
    "epsilon = norm.rvs(0, sigma, size=n)\n",
    "\n",
    "# Calculate the outcome variable Y\n",
    "Y = beta_0 + beta_1 * x + epsilon\n",
    "\n",
    "# Create a DataFrame for easier plotting with Plotly\n",
    "df = pd.DataFrame({'x': x, 'Y': Y})\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=Y, mode='markers', name='Simulated Data', marker=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=x, y=beta_0 + beta_1 * x, mode='lines', name='True Line', line=dict(color='red')))\n",
    "fig.update_layout(title=\"Simple Linear Model: True Line and Simulated Data\", xaxis_title=\"Predictor (x)\", yaxis_title=\"Outcome (Y)\")\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "- **Data Generation**: `x` values are sampled uniformly between 0 and 10, while the error `epsilon` is generated from a normal distribution with mean 0 and standard deviation \\( \\sigma \\).\n",
    "- **Outcome Calculation**: The outcome \\( Y \\) is computed using the theoretical model \\( Y = \\beta_0 + \\beta_1 x + \\epsilon \\).\n",
    "- **Visualization**: We use Plotly to create an interactive plot where the red line represents the theoretical line \\( \\beta_0 + \\beta_1 x \\) and the blue points show the simulated \\( Y \\) values around this line.\n",
    "\n",
    "### Intuitive Interpretation of the Result\n",
    "\n",
    "In this plot, the **red line** represents the underlying relationship dictated by the slope (\\( \\beta_1 \\)) and intercept (\\( \\beta_0 \\)). The **blue points** scatter around this line, showing the natural variability (error) that is part of any real-world data, captured by \\( \\epsilon \\). This variability shows that, while the model assumes a linear relationship, individual outcomes \\( Y_i \\) vary around the line due to random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607e6c2",
   "metadata": {},
   "source": [
    "**history**:https://chatgpt.com/share/672bc69d-7ea4-8003-acce-9bcc37bc4347"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6716017",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bd2cc",
   "metadata": {},
   "source": [
    "Here’s a breakdown of the code, focusing on answering each of the inline questions with explanations for creating and visualizing a fitted Simple Linear Regression model.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "```python\n",
    "import statsmodels.formula.api as smf  # what is this library for?\n",
    "```\n",
    "- **Answer**: `statsmodels.formula.api` (abbreviated as `smf`) is a library for specifying and fitting statistical models, particularly for linear and other regression models. It provides an easy-to-use formula interface (similar to R) for defining models, like `ols` for Ordinary Least Squares regression.\n",
    "\n",
    "```python\n",
    "import plotly.express as px  # this is a plotting library\n",
    "```\n",
    "- **Answer**: `plotly.express` (`px`) is a high-level plotting library that allows for quick, easy data visualizations with concise syntax. It’s widely used for scatter plots, line charts, and other common graphs, and supports a trendline option for regression.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Model Fitting\n",
    "\n",
    "```python\n",
    "model_data_specification = smf.ols(\"Y~x\", data=df)  # what are the following two steps doing?\n",
    "```\n",
    "- **Answer**: This line specifies the model, with `Y~x` indicating that `Y` is the dependent variable, and `x` is the independent variable. The formula uses `ols` (Ordinary Least Squares), which minimizes the sum of squared residuals to fit the line.\n",
    "\n",
    "```python\n",
    "fitted_model = model_data_specification.fit()\n",
    "```\n",
    "- **Answer**: This line fits the model to the data provided in `df`, computing the regression parameters (slope and intercept) for the best fit. The `fitted_model` object now contains the results of the regression, including summary statistics and fitted values.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Summary and Outputs\n",
    "\n",
    "```python\n",
    "fitted_model.summary()  # simple explanation?\n",
    "```\n",
    "- **Answer**: This provides a comprehensive summary of the fitted regression model. It includes details such as coefficients, standard errors, p-values, R-squared value, and statistical test results, helping evaluate the model’s fit and significance.\n",
    "\n",
    "```python\n",
    "fitted_model.summary().tables[1]  # simple explanation?\n",
    "```\n",
    "- **Answer**: This isolates the second table within the summary, which typically contains the regression coefficients (intercept and slope), standard errors, t-statistics, and p-values for the model parameters.\n",
    "\n",
    "```python\n",
    "fitted_model.params  # simple explanation?\n",
    "```\n",
    "- **Answer**: This returns the estimated parameters (intercept and slope) of the regression model. It’s useful if you only need the coefficients without other summary details.\n",
    "\n",
    "```python\n",
    "fitted_model.params.values  # simple explanation?\n",
    "```\n",
    "- **Answer**: This returns the parameter values (intercept and slope) in an array format, which can be convenient for calculations or further analysis.\n",
    "\n",
    "```python\n",
    "fitted_model.rsquared  # simple explanation?\n",
    "```\n",
    "- **Answer**: This returns the R-squared value, a measure of how well the model explains the variability in the data. It ranges from 0 to 1, with values closer to 1 indicating a better fit.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization\n",
    "\n",
    "```python\n",
    "df['Data'] = 'Data'  # hack to add data to legend\n",
    "```\n",
    "- **Answer**: This adds a new column, `Data`, with a constant value of 'Data' to each row in `df`. It’s a trick to ensure that when plotting, the scatter points have a label in the legend.\n",
    "\n",
    "```python\n",
    "fig = px.scatter(df, x='x',  y='Y', color='Data', trendline='ols', title='Y vs. x')\n",
    "```\n",
    "- **Answer**: This creates a scatter plot of `Y` vs. `x` with Plotly Express, with a fitted regression line (using `trendline='ols'`). The color option ensures the data points are labeled 'Data' in the legend, and the title labels the graph.\n",
    "\n",
    "```python\n",
    "fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues, line=dict(color='blue'), name=\"trendline='ols'\")\n",
    "```\n",
    "- **Answer**: This adds the fitted regression line to the scatter plot. The `fittedvalues` from `fitted_model` represent the predicted Y values for each x. The color is set to blue, and the legend entry is labeled `\"trendline='ols'\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization Output\n",
    "\n",
    "```python\n",
    "fig.show()  # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "```\n",
    "- **Answer**: This displays the figure with the data points and regression line. For GitHub and MarkUs, use `fig.show(renderer=\"png\")` to ensure compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Sessions with Chatbot\n",
    "\n",
    "Remember to save a summary of your interactions with the chatbot, as these could be useful for later review or submission. If using a specific course chatbot, refer to any session summaries directly in your notebook for easy reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266d755",
   "metadata": {},
   "source": [
    "history :https://chatgpt.com/share/672bc99e-9f08-8003-b6d4-83c9e0d9cb5e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0502d92",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bea1e7",
   "metadata": {},
   "source": [
    "This setup demonstrates and contrasts two ways of adding linear regression lines to a scatter plot:\n",
    "\n",
    "1. **Automatic Fitting with `plotly.express`**:\n",
    "   Using `px.scatter` with `trendline='ols'` provides a quick way to add an Ordinary Least Squares (OLS) regression line to the scatter plot. Here, `plotly.express` automatically fits the model to the data and visualizes the trendline without requiring manual calculations of the line parameters (`beta0` and `beta1`).\n",
    "\n",
    "2. **Manual Line Addition with `plotly.graph_objects`**:\n",
    "   By setting up a figure with `plotly.graph_objects`, you can add specific traces without automatic fitting, allowing greater control. For example, here you first plot the raw data points with `fig.add_trace(go.Scatter(...))`. Then, you manually add a regression line by defining the x-range (`x_range`) and calculating the corresponding y-values using `beta0` and `beta1`, which are assumed to be estimated parameters from a model (for instance, using `statsmodels`). This line is displayed with a custom appearance (e.g., a dotted orange line).\n",
    "\n",
    "### Purpose of the Comparison\n",
    "The combined code shows the difference in approach between:\n",
    "- Automatically using `plotly.express` to fit and plot a trendline directly based on your dataset.\n",
    "- Manually adding a line with `plotly.graph_objects`, which can be tailored to specific parameter values (`beta0` and `beta1`), possibly from a theoretical model or a different context.\n",
    "\n",
    "### Visualization and Insight through Simulation\n",
    "A powerful way to interactively visualize this is by repeatedly simulating datasets and plotting both the automatic and manual regression lines, which allows you to observe variations in fitted vs. specified lines. This repeated visualization helps in understanding the fit's consistency or deviations in different simulations. \n",
    "\n",
    "### Code Addendum for Contrast\n",
    "The additional code demonstrates manual control by appending a line (`y_line = beta0 + beta1 * x_range`) to the plot. This shows the line calculated directly from specified `beta0` and `beta1`, contrasting with the automatically fitted line, and giving insight into how both can be visualized and interpreted on the same plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa65d9",
   "metadata": {},
   "source": [
    "Here's a summary of the recent topics we've covered in our sessions:\n",
    "\n",
    "1. **Python Functions for Analysis**:\n",
    "   - We worked on functions to scale grades and calculate word lengths, exploring ways to design these functions and add clear explanations, including examples for testing.\n",
    "   - You developed a function to calculate weighted grades from lists of marks and weights, as well as a function to calculate employee pay based on time and rate.\n",
    "\n",
    "2. **Data Exploration with Titanic and Animal Crossing Datasets**:\n",
    "   - We explored initial summaries and analyses on these datasets, using methods like `df.describe()` and `df['column'].value_counts()` to understand the data’s structure and contents.\n",
    "   - You practiced handling missing data with `del df['col']` and used group-by summaries to analyze relationships within the data.\n",
    "\n",
    "3. **Statistical Analysis and Hypothesis Testing**:\n",
    "   - For your proposal, we discussed possible methods (like one-sample and two-sample hypothesis tests and simple linear regression) to analyze health-related variables.\n",
    "   - We used `statsmodels` for OLS regression and explored interpreting regression summaries (e.g., `params`, `rsquared`, and `summary.tables`).\n",
    "\n",
    "4. **Python and Simulation Projects**:\n",
    "   - You created a simulation for the Monty Hall problem, enhancing your understanding of probabilities in programming.\n",
    "   - We also looked at calculating the time complexity of `for` loops and how to analyze iteration steps.\n",
    "\n",
    "5. **Visualizations and Plotting with Plotly**:\n",
    "   - We compared `plotly.express` and `plotly.graph_objects`, focusing on how `px.scatter` with `trendline='ols'` adds an automatic fitted model, whereas `plotly.graph_objects` offers precise control for manual additions.\n",
    "   - We discussed techniques for using `plotly.graph_objects` when you need to avoid automatic model fitting.\n",
    "\n",
    "6. **Proposal Writing and Data Science Goals**:\n",
    "   - We discussed how to structure a proposal that covers data description, planned analyses, potential outcomes, and relevance to course objectives. This included defining variables, setting analysis goals, and forecasting results based on statistical tests.\n",
    "\n",
    "7. **General Python Coding Skills and Testing**:\n",
    "   - We worked on simplifying code implementations, especially for new Python users, and how to design clear and manageable function bodies.\n",
    "   - We covered `pytest` and structuring assertions within tests, focusing on how to design the expected output part of each test.\n",
    "\n",
    "8. **Visualization Techniques and Interpretation**:\n",
    "   - You shared a preference for box plots and specific rendering methods (e.g., using `fig.show(renderer=\"png\")` for non-interactive environments).\n",
    "   - We discussed setting up simulations to observe repeated visualizations and trends, providing insight into model consistency across different datasets.\n",
    "\n",
    "If you need more details on any of these topics or guidance on new topics, just let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f67b51",
   "metadata": {},
   "source": [
    "**history** :https://chatgpt.com/share/672bcb7f-89a4-8003-8514-fef090b23371"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0aa01",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1b0f3",
   "metadata": {},
   "source": [
    "To understand how `fitted_model.fittedvalues` are derived, let's look at how they relate to the model's fitted values from the regression summary.\n",
    "\n",
    "### Background: True vs. Fitted Linear Models\n",
    "\n",
    "In simple linear regression, the model's true theoretical form for predicting \\( Y_i \\) from \\( x_i \\) (including the random error term \\( \\epsilon_i \\)) is:\n",
    "\n",
    "\\[\n",
    "Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n",
    "\\]\n",
    "\n",
    "Here:\n",
    "- \\( \\beta_0 \\) is the true intercept.\n",
    "- \\( \\beta_1 \\) is the true slope.\n",
    "- \\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma) \\) represents the error term for each observation.\n",
    "\n",
    "However, since we generally don’t know the true parameters \\( \\beta_0 \\) and \\( \\beta_1 \\), we estimate them using data. This yields estimated parameters \\( \\hat{\\beta}_0 \\) and \\( \\hat{\\beta}_1 \\), which form our fitted model for \"in-sample predictions\":\n",
    "\n",
    "\\[\n",
    "\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n",
    "\\]\n",
    "\n",
    "In this fitted model:\n",
    "- \\( \\hat{y}_i \\) is the predicted (fitted) value for \\( Y \\) based on \\( x_i \\).\n",
    "- \\( \\hat{\\beta}_0 \\) and \\( \\hat{\\beta}_1 \\) are obtained from the linear regression output (accessible in Python via `fitted_model.params`).\n",
    "\n",
    "### Deriving Fitted Values from Estimated Parameters\n",
    "\n",
    "1. **Retrieve Estimated Coefficients**: When you call `fitted_model.summary().tables[1]`, you’ll see the coefficients in the table, specifically \\( \\hat{\\beta}_0 \\) (intercept) and \\( \\hat{\\beta}_1 \\) (slope for \\( x \\)). These coefficients are also stored in `fitted_model.params`, where `fitted_model.params[0]` is the intercept \\( \\hat{\\beta}_0 \\), and `fitted_model.params[1]` is the slope \\( \\hat{\\beta}_1 \\).\n",
    "\n",
    "2. **Calculate Fitted Values**: The fitted values \\( \\hat{y}_i \\) are calculated by substituting each observed \\( x_i \\) into the fitted linear equation:\n",
    "\n",
    "   \\[\n",
    "   \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n",
    "   \\]\n",
    "\n",
    "   This means that for each observation, `fitted_model.fittedvalues[i]` is computed as:\n",
    "\n",
    "   ```python\n",
    "   fitted_model.fittedvalues[i] = fitted_model.params[0] + fitted_model.params[1] * x_i\n",
    "   ```\n",
    "\n",
    "   The library handles this calculation automatically for each \\( x_i \\) in your data, creating a list of fitted values.\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Suppose you have a model `fitted_model` where `fitted_model.params[0] = 2.5` (intercept) and `fitted_model.params[1] = 1.3` (slope). For an input \\( x_i = 5 \\):\n",
    "\n",
    "\\[\n",
    "\\hat{y}_i = 2.5 + 1.3 \\times 5 = 2.5 + 6.5 = 9.0\n",
    "\\]\n",
    "\n",
    "This \\( \\hat{y}_i = 9.0 \\) will appear in `fitted_model.fittedvalues` as the prediction for \\( x_i = 5 \\).\n",
    "\n",
    "### Summary\n",
    "\n",
    "The process to obtain `fitted_model.fittedvalues` involves:\n",
    "1. Estimating the coefficients \\( \\hat{\\beta}_0 \\) and \\( \\hat{\\beta}_1 \\) through regression.\n",
    "2. Plugging each \\( x_i \\) into the fitted model \\( \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\) using these coefficients.\n",
    "3. Automatically storing these fitted values in `fitted_model.fittedvalues`.\n",
    "\n",
    "This approach creates an in-sample prediction model that approximates the theoretical true model, allowing you to assess predictions based solely on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea9c83",
   "metadata": {},
   "source": [
    "**history** https://chatgpt.com/share/672bcc5b-8898-8003-89e4-a7709d172117"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80ab5c",
   "metadata": {},
   "source": [
    " # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfffb92",
   "metadata": {},
   "source": [
    "The \"ordinary least squares\" (OLS) method finds the line that minimizes the total sum of the squared differences between each observed data point's \\( Y_i \\) value and its corresponding predicted value \\( \\hat{y}_i \\) from the line. This difference is called the residual, \\( e_i = Y_i - \\hat{y}_i \\).\n",
    "\n",
    "The reason OLS uses the square of these residuals is to penalize larger differences more heavily, ensuring that the line found represents the closest overall fit to the data in a way that balances all points effectively. Squaring the residuals also ensures they are all positive, so that residuals above and below the line don’t cancel each other out.\n",
    "\n",
    "In the provided plot:\n",
    "\n",
    "- The blue line represents the trendline fitted by the OLS method (`trendline='ols'`), while the orange dashed line represents the true model, \\( Y = \\beta_0 + \\beta_1 x \\).\n",
    "- Red vertical dashed lines show each residual, which is the vertical distance from each actual point to the fitted line.\n",
    "- The horizontal black dashed line represents \\( \\bar{Y} \\), the mean of the \\( Y \\) values, to give context to the spread of the data around the mean.\n",
    "\n",
    "By minimizing the sum of the squared residuals, the OLS method ensures this blue line is positioned to have the smallest total distance to all points compared to any other line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7994b2",
   "metadata": {},
   "source": [
    "**history** :https://chatgpt.com/share/672bcd1e-4470-8003-87f2-0c78dd6b80c6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cfac38",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192aeaa",
   "metadata": {},
   "source": [
    "In the context of simple linear regression, the goal is to quantify how well the fitted model (with predictions for \\( Y \\)) explains the variation in the observed outcome \\( Y \\). Here's an explanation for each of the four expressions and how they relate to this concept:\n",
    "\n",
    "1. **Expression 1:**\n",
    "   \\[\n",
    "   1 - \\frac{\\sum_{i=1}^n (Y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}\n",
    "   \\]\n",
    "   - This expression calculates the proportion of the total variation in \\( Y \\) that is explained by the fitted model. The numerator represents the sum of the squared residuals (i.e., the squared differences between the observed values \\( Y \\) and the predicted values \\( \\hat{y}_i \\)), while the denominator represents the total sum of squares, which is the squared distance between each observed value \\( Y_i \\) and the mean of \\( Y \\), \\( \\bar{Y} \\). \n",
    "   - The result is a measure of how well the model's predictions (\\( \\hat{y}_i \\)) reduce the variability compared to the original data (\\( Y \\)).\n",
    "   - This is exactly the definition of **R-squared**, the proportion of the variation in \\( Y \\) that is explained by the model.\n",
    "\n",
    "2. **Expression 2:**\n",
    "   \\[\n",
    "   \\text{fitted_model.rsquared}\n",
    "   \\]\n",
    "   - This is the built-in attribute in many regression models (like `statsmodels`) that directly returns the R-squared value. It quantifies the proportion of variation in the outcome \\( Y \\) explained by the fitted model. The R-squared value is used as a standard measure of the model's accuracy, with values closer to 1 indicating a better fit, and values closer to 0 indicating that the model explains little of the variation.\n",
    "\n",
    "3. **Expression 3:**\n",
    "   \\[\n",
    "   \\text{np.corrcoef}(Y, \\text{fitted_model.fittedvalues})[0,1]^2\n",
    "   \\]\n",
    "   - This expression computes the squared correlation coefficient between the observed data \\( Y \\) and the fitted values from the model. The correlation coefficient measures the strength and direction of the linear relationship between these two variables.\n",
    "   - The square of the correlation coefficient, \\( r^2 \\), is exactly the definition of **R-squared**, which tells you how much of the variation in \\( Y \\) can be explained by the fitted model.\n",
    "   - In this context, this correlation measures the linearity between the observed values and their predictions, with \\( r^2 \\) reflecting the proportion of explained variation.\n",
    "\n",
    "4. **Expression 4:**\n",
    "   \\[\n",
    "   \\text{np.corrcoef}(Y, x)[0,1]^2\n",
    "   \\]\n",
    "   - This expression calculates the squared correlation coefficient between the observed data \\( Y \\) and the independent variable \\( x \\) (used to predict \\( Y \\) in the model). \n",
    "   - This coefficient measures the strength and direction of the linear relationship between \\( Y \\) and \\( x \\). The squared value, \\( r^2 \\), captures the proportion of the variation in \\( Y \\) that can be attributed to \\( x \\), rather than other factors.\n",
    "   - This expression is useful when you want to understand how strongly the predictor \\( x \\) is related to the outcome \\( Y \\), independently of the fitted model.\n",
    "\n",
    "### Summary:\n",
    "- **R-squared** (from both the manual calculation and `fitted_model.rsquared`) is a key metric that reflects the proportion of the variation in \\( Y \\) explained by the model, i.e., how well the model fits the data.\n",
    "- The squared correlation between \\( Y \\) and the fitted values (from `np.corrcoef(...)`) also captures this proportion of explained variation, while the squared correlation between \\( Y \\) and \\( x \\) measures how well the predictor explains the outcome \\( Y \\) before fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f374b4",
   "metadata": {},
   "source": [
    "**history**:https://chatgpt.com/share/672bcda7-c248-8003-9246-5398c16671d7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a00c6",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8486fb",
   "metadata": {},
   "source": [
    "The Simple Linear Regression (SLR) model assumes several things to provide reliable estimates. Let's break them down in the context of your provided code and data:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variable (Amount of Fertilizer) and the dependent variable (Crop Yield) is assumed to be linear. This means that the crop yield increases or decreases proportionally as the amount of fertilizer changes. You can visually inspect the scatter plot to check if this assumption holds.\n",
    "\n",
    "2. **Independence**: The residuals (the differences between the observed values and the predicted values) should be independent of each other. In your code, this assumption is not directly checked but can be inferred by looking at the residuals' distribution. If there are patterns in the residuals, this assumption might be violated (e.g., if residuals are correlated with the amount of fertilizer).\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals should be constant across all levels of the independent variable. In the histogram of residuals, if the residuals are spread evenly across the range of predicted values, this assumption is met. If the residuals fan out or contract in a pattern, it could indicate heteroscedasticity, where the variance of residuals changes.\n",
    "\n",
    "4. **Normality of residuals**: The residuals should be approximately normally distributed, especially when you want to perform hypothesis testing on the coefficients. You can check for normality by examining the histogram of the residuals. If the residuals are not normal, transformations (e.g., log-transformation) might be needed to meet this assumption.\n",
    "\n",
    "5. **No multicollinearity**: Although not relevant in your example (since you have only one predictor), this assumption generally refers to the absence of strong correlations between independent variables in multiple regression. In simple linear regression, this assumption does not apply, as there is only one predictor.\n",
    "\n",
    "In your case, by generating the scatter plot and histogram of residuals, you can get an idea of whether these assumptions are met. If the residuals appear randomly scattered with no clear pattern in the plot and are approximately normally distributed in the histogram, the assumptions are likely satisfied. However, if you see patterns in the residuals (e.g., a funnel shape), it might suggest a problem with the assumptions, particularly with homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f3168",
   "metadata": {},
   "source": [
    "**history**: https://chatgpt.com/share/672bcf04-ff20-8003-9f8b-80b63f4321a7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a583e5b",
   "metadata": {},
   "source": [
    " # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42b47d",
   "metadata": {},
   "source": [
    "In the context of a Simple Linear Regression model, the null hypothesis of \"no linear association (on average)\" between the two variables can be stated as follows:\n",
    "\n",
    "**Null Hypothesis (H₀):** There is no linear association between the \"duration\" of the eruption and the \"waiting\" time between eruptions. Mathematically, this can be written as:\n",
    "\n",
    "\\[\n",
    "H_0: \\beta_1 = 0\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\beta_1 \\) is the slope of the regression line, which represents the change in the dependent variable (duration) for a one-unit change in the independent variable (waiting).\n",
    "\n",
    "The alternative hypothesis (H₁) would be:\n",
    "\n",
    "\\[\n",
    "H_1: \\beta_1 \\neq 0\n",
    "\\]\n",
    "\n",
    "This suggests that there is a linear relationship between the variables.\n",
    "\n",
    "### Interpreting the Results\n",
    "\n",
    "To test the null hypothesis and interpret the evidence in the data, we use the output from `fitted_model.summary()`, which will provide the estimated value for \\( \\beta_1 \\), along with the associated p-value.\n",
    "\n",
    "The steps to interpret the results:\n",
    "\n",
    "1. **Look at the coefficient for \"waiting\" (slope, \\( \\beta_1 \\)):**\n",
    "   - This will give you the estimated change in \"duration\" for each additional unit of \"waiting\".\n",
    "   \n",
    "2. **Examine the p-value for \\( \\beta_1 \\):**\n",
    "   - If the p-value is smaller than the significance level (commonly 0.05), you reject the null hypothesis and conclude that there is evidence of a linear association between \"waiting\" and \"duration\".\n",
    "   - If the p-value is larger than 0.05, you fail to reject the null hypothesis, meaning there isn't enough evidence to claim a significant linear association between the two variables.\n",
    "\n",
    "### Example Interpretation (hypothetical output):\n",
    "\n",
    "If the fitted model output looks something like this:\n",
    "\n",
    "```\n",
    "                            OLS Regression Results\n",
    "==============================================================================\n",
    "Dep. Variable:                duration   R-squared:                       0.452\n",
    "Model:                            OLS   Adj. R-squared:                  0.447\n",
    "Method:                 Least Squares   F-statistic:                     98.72\n",
    "Date:                Mon, 06 Nov 2024   Prob (F-statistic):           1.23e-17\n",
    "Time:                        10:30:00   Log-Likelihood:                -312.67\n",
    "No. Observations:                 272   AIC:                             631.34\n",
    "Df Residuals:                     270   BIC:                             636.56\n",
    "Df Model:                           1\n",
    "Covariance Type:            nonrobust\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "Intercept     53.3564      3.358     15.911      0.000      46.763      59.950\n",
    "waiting        0.0785      0.008      9.936      0.000       0.063       0.094\n",
    "==============================================================================\n",
    "```\n",
    "\n",
    "- **Intercept (constant):** \\( 53.3564 \\) (This represents the expected duration when the waiting time is zero.)\n",
    "- **Slope (waiting):** \\( 0.0785 \\) (This means that for each additional minute of waiting time, the duration of the eruption increases by about 0.0785 minutes.)\n",
    "- **p-value for \"waiting\":** 0.000 (Since this p-value is much smaller than 0.05, we reject the null hypothesis that \\( \\beta_1 = 0 \\) and conclude that there is a significant linear association between waiting time and eruption duration.)\n",
    "\n",
    "### Beliefs Regarding the Data\n",
    "\n",
    "Given this output, we would conclude that there is strong evidence against the null hypothesis. Specifically, there is a significant linear relationship between the waiting time between eruptions and the duration of the eruption. As the waiting time increases, the eruption duration tends to increase as well, on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b862be",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416fe97",
   "metadata": {},
   "source": [
    "To assess the evidence against the null hypothesis, you're running an Ordinary Least Squares (OLS) regression for the relationship between the geyser eruption duration (`duration`) and wait time (`waiting`) under the condition that the waiting time is less than a specified limit (`short_wait_limit`).\n",
    "\n",
    "Here’s how to interpret the results:\n",
    "\n",
    "1. **Hypothesis Testing**:\n",
    "   - The null hypothesis (\\(H_0\\)) for this regression is that there is no relationship between the wait time (`waiting`) and the duration of the eruption (`duration`). Mathematically, this is:\n",
    "     \\[\n",
    "     H_0: \\beta_1 = 0\n",
    "     \\]\n",
    "     where \\(\\beta_1\\) is the slope coefficient for the `waiting` variable. If the p-value for `waiting` is small (usually less than 0.05), you can reject the null hypothesis and conclude that there is evidence for a relationship between wait time and eruption duration.\n",
    "\n",
    "2. **Code Execution**:\n",
    "   The line `smf.ols('duration ~ waiting', data=old_faithful[short_wait]).fit()` fits the regression model for the subset of the dataset where the waiting time is less than the specified `short_wait_limit`. The summary statistics printed by `tables[1]` will provide coefficients and p-values.\n",
    "\n",
    "3. **Key Outputs to Look For**:\n",
    "   - **Coefficient for `waiting`**: This shows the slope of the relationship between wait time and eruption duration. A positive value indicates that as wait time increases, eruption duration increases.\n",
    "   - **p-value for `waiting`**: If this p-value is less than 0.05, it suggests strong evidence against the null hypothesis (i.e., there is a statistically significant relationship between the two variables).\n",
    "   - **R-squared value**: This tells you how much of the variation in eruption duration is explained by the wait time. The higher the R-squared, the better the model fits the data.\n",
    "\n",
    "4. **Scatter Plot**:\n",
    "   - The scatter plot created by `px.scatter` with a trendline (`trendline='ols'`) will visually show the relationship. If the trendline shows a clear upward or downward slope, it indicates a relationship between wait time and eruption duration.\n",
    "\n",
    "To conduct the analysis for different `short_wait_limit` values (62, 64, 66), you can adjust the `short_wait_limit` value and run the code again. Each change will filter the dataset to different subsets of short wait times and test if the relationship between `waiting` and `duration` holds.\n",
    "\n",
    "### Example Results Interpretation:\n",
    "- If for `short_wait_limit = 62`, the p-value for `waiting` is 0.01, you would reject the null hypothesis and conclude that there is evidence of a relationship.\n",
    "- If for `short_wait_limit = 66`, the p-value for `waiting` is 0.10, you would fail to reject the null hypothesis, suggesting there’s no strong evidence for a relationship under this condition.\n",
    "\n",
    "This allows you to explore how the relationship changes depending on the limit of short wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab346dd0",
   "metadata": {},
   "source": [
    "# 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769b039",
   "metadata": {},
   "source": [
    "To achieve your objective of bootstrapping Simple Linear Regression models for long wait times, simulating samples under a null hypothesis, and visualizing the sampling distribution of the fitted slope coefficients, here's the code that follows the steps outlined:\n",
    "\n",
    "### Steps in the Code:\n",
    "1. **Create Bootstrapped Simple Linear Regression Models**:\n",
    "   - We'll generate multiple bootstrap samples from the dataset of long wait times.\n",
    "   - For each bootstrap sample, we fit a Simple Linear Regression model and collect the fitted slope coefficients.\n",
    "\n",
    "2. **Simulate Samples Under the Null Hypothesis**:\n",
    "   - We'll simulate new data where \\( \\beta_0 = 1.65 \\), \\( \\beta_1 = 0 \\), and \\( \\sigma = 0.37 \\).\n",
    "   - The simulated data will assume no linear relationship between waiting time and duration (i.e., \\( \\beta_1 = 0 \\)).\n",
    "\n",
    "3. **Calculate Bootstrapped Confidence Intervals**:\n",
    "   - From the bootstrapped sampling distribution, we'll compute a 95% confidence interval for the slope coefficients.\n",
    "   - We'll also calculate a p-value for the slope coefficient under the null hypothesis.\n",
    "\n",
    "### Code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "\n",
    "# Load the Old Faithful dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Restrict to long wait times (assuming long_wait is already defined)\n",
    "long_wait_limit = 70  # Assuming 70 as the long wait time threshold\n",
    "long_wait = old_faithful.waiting >= long_wait_limit\n",
    "\n",
    "# 1. Create Bootstrapped Simple Linear Regression Models\n",
    "n_bootstrap = 1000\n",
    "bootstrapped_slope_coefficients = []\n",
    "\n",
    "# For each bootstrap sample\n",
    "for _ in range(n_bootstrap):\n",
    "    # Create a bootstrap sample with replacement\n",
    "    bootstrap_sample = old_faithful[long_wait].sample(n=long_wait.sum(), replace=True)\n",
    "    # Fit a Simple Linear Regression model\n",
    "    model = smf.ols('duration ~ waiting', data=bootstrap_sample).fit()\n",
    "    # Collect the fitted slope coefficient\n",
    "    bootstrapped_slope_coefficients.append(model.params[1])\n",
    "\n",
    "# Visualize the Bootstrapped Sampling Distribution of the Slope Coefficients\n",
    "fig = px.histogram(bootstrapped_slope_coefficients, nbins=50,\n",
    "                   title=\"Bootstrapped Sampling Distribution of the Slope Coefficient\",\n",
    "                   labels={'value': 'Slope Coefficient'})\n",
    "fig.show()\n",
    "\n",
    "# 2. Simulate Samples Under the Null Hypothesis\n",
    "n_simulations = 1000\n",
    "simulated_slope_coefficients = []\n",
    "\n",
    "# For each simulation\n",
    "for _ in range(n_simulations):\n",
    "    # Create a simulated dataset under the null hypothesis (β0=1.65, β1=0)\n",
    "    old_faithful_simulation = old_faithful[long_wait].copy()\n",
    "    old_faithful_simulation['duration'] = 1.65 + 0 * old_faithful_simulation['waiting'] + \\\n",
    "                                           stats.norm(loc=0, scale=0.37).rvs(size=long_wait.sum())\n",
    "    \n",
    "    # Fit a Simple Linear Regression model to the simulated data\n",
    "    model_simulation = smf.ols('duration ~ waiting', data=old_faithful_simulation).fit()\n",
    "    # Collect the fitted slope coefficient\n",
    "    simulated_slope_coefficients.append(model_simulation.params[1])\n",
    "\n",
    "# Visualize the Simulated Sampling Distribution of the Slope Coefficient Under the Null Hypothesis\n",
    "fig = px.histogram(simulated_slope_coefficients, nbins=50,\n",
    "                   title=\"Simulated Sampling Distribution of the Slope Coefficient (Null Hypothesis)\",\n",
    "                   labels={'value': 'Slope Coefficient'})\n",
    "fig.show()\n",
    "\n",
    "# 3. Calculate 95% Bootstrapped Confidence Interval\n",
    "bootstrapped_ci = np.quantile(bootstrapped_slope_coefficients, [0.025, 0.975])\n",
    "print(f\"95% Bootstrapped Confidence Interval for the Slope Coefficient: {bootstrapped_ci}\")\n",
    "\n",
    "# 4. Calculate the p-value from the Simulated Sampling Distribution\n",
    "observed_slope = smf.ols('duration ~ waiting', data=old_faithful[long_wait]).fit().params[1]\n",
    "simulated_p_value = (np.abs(simulated_slope_coefficients) >= np.abs(observed_slope)).mean()\n",
    "print(f\"Simulated p-value: {simulated_p_value}\")\n",
    "\n",
    "# 5. Check if the p-value and 95% confidence interval match the significance test from OLS\n",
    "# This is where you can compare to a standard OLS p-value or test statistic (e.g., from model.summary())\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Bootstrap Sampling**:\n",
    "   - The code generates 1000 bootstrap samples by randomly sampling from the dataset of long wait times (`long_wait`).\n",
    "   - For each sample, a Simple Linear Regression model is fitted, and the slope coefficient is stored in `bootstrapped_slope_coefficients`.\n",
    "\n",
    "2. **Simulating Under the Null Hypothesis**:\n",
    "   - A new dataset is simulated with \\( \\beta_1 = 0 \\), meaning there's no linear association between `waiting` and `duration` (null hypothesis).\n",
    "   - The `duration` is simulated using the given normal distribution parameters, and the slope coefficients from Simple Linear Regression are stored in `simulated_slope_coefficients`.\n",
    "\n",
    "3. **Visualizing the Distributions**:\n",
    "   - The distributions of the bootstrapped and simulated slope coefficients are visualized using `plotly.express.histogram`.\n",
    "\n",
    "4. **Confidence Interval and p-value**:\n",
    "   - The 95% confidence interval for the slope coefficient is calculated from the bootstrapped samples.\n",
    "   - The p-value is calculated based on the simulated slope coefficients. It checks if the observed slope (from the real data) is significantly different from zero under the null hypothesis.\n",
    "\n",
    "### Conclusion:\n",
    "This code provides a way to compare the bootstrapped confidence interval for the slope coefficient with the p-value obtained from simulating under the null hypothesis. It helps assess if the observed relationship in the data is statistically significant, considering the null hypothesis assumption of no linear association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba2c73",
   "metadata": {},
   "source": [
    "history https://chatgpt.com/share/672bdfbc-4f74-8003-b438-7d744a7e4fca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41504c50",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18224be",
   "metadata": {},
   "source": [
    "In this case, we are switching from a continuous predictor (waiting time) to a categorical predictor using an indicator (or dummy) variable for the wait time category. The key changes in this approach and the \"big picture\" differences are as follows:\n",
    "\n",
    "### Key Model Differences:\n",
    "\n",
    "1. **Simple Linear Regression Model**:\n",
    "   - **Original Specification**: The models you initially considered are of the form `duration ~ waiting`, which assumes a continuous relationship between the `waiting` time (a numeric predictor) and the `duration` of the geyser eruption.\n",
    "   \n",
    "   - **New Specification**: You are now considering a model of the form:\n",
    "     \\[\n",
    "     Y_i = \\beta_0 + 1[\\text{long}](k_i)\\beta_{\\text{contrast}} + \\epsilon_i\n",
    "     \\]\n",
    "     where \\( k_i \\) is a categorical variable (indicator) representing the wait time category (either \"short\" or \"long\"). In this model, the predictor is no longer continuous but categorical, and we are comparing the **mean duration** between these two groups.\n",
    "\n",
    "2. **Indicator Variable (Dummy Variable)**:\n",
    "   - In the new model, we use an **indicator variable** (also known as a dummy variable) that represents whether a given observation is from the \"short\" wait time group or the \"long\" wait time group.\n",
    "   - The `C(kind, Treatment(reference=\"short\"))` part in the formula is a way of specifying the **categorical nature** of the `kind` variable and treating \"short\" as the reference category (i.e., the group against which \"long\" will be compared).\n",
    "   \n",
    "   - This approach allows the model to estimate **two different intercepts**: one for the \"short\" group (which is the baseline, since it's the reference) and one for the \"long\" group. The difference in the intercepts between these two groups reflects the **contrast** or difference in their mean durations.\n",
    "\n",
    "### Big Picture Differences:\n",
    "- **Continuous vs. Categorical Predictor**:\n",
    "  - **Earlier models** (like `duration ~ waiting`) treated the `waiting` time as a continuous variable, meaning the model assumes a smooth relationship between waiting time and duration.\n",
    "  - In the new specification with `duration ~ C(kind)`, we treat \"kind\" as a categorical variable, meaning we're comparing two distinct groups (short vs. long wait times), and there is no assumption of a continuous relationship. The model essentially tests if there's a significant difference between the two groups' mean durations.\n",
    "\n",
    "- **Hypothesis Testing**:\n",
    "  - In the original continuous models, you were interested in the slope of the relationship between waiting time and eruption duration.\n",
    "  - In the new model, you are testing whether there is a **statistically significant difference** in the average eruption duration between the two groups (\"short\" vs. \"long\" wait times), which is a comparison of group means rather than a relationship between two continuous variables.\n",
    "\n",
    "### Hypothesis Against the Null:\n",
    "The null hypothesis in the new model is that there is **no difference between the groups** in terms of their average eruption duration, which can be tested by checking the **contrast coefficient** in the regression output. The p-value associated with this contrast will indicate whether there is sufficient evidence to reject the null hypothesis and conclude that the group means are significantly different.\n",
    "\n",
    "### Code to Estimate the Model and Display Results:\n",
    "\n",
    "```python\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Fit the model using 'kind' as the categorical variable with \"short\" as the reference group\n",
    "model = smf.ols('duration ~ C(kind, Treatment(reference=\"short\"))', data=old_faithful).fit()\n",
    "\n",
    "# Display the model summary\n",
    "display(model.summary().tables[1])\n",
    "\n",
    "# Create a box plot to visualize the difference in duration between \"short\" and \"long\" wait times\n",
    "fig = px.box(old_faithful, x='kind', y='duration', \n",
    "             title='duration ~ kind',\n",
    "             category_orders={'kind': ['short', 'long']})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **Model Estimation**: \n",
    "   - The `smf.ols` function fits an Ordinary Least Squares regression with `C(kind, Treatment(reference=\"short\"))`, where `kind` is treated as a categorical variable and \"short\" is used as the reference category.\n",
    "   - This model will produce a coefficient for the \"long\" category, which represents the difference in mean eruption duration between \"long\" and \"short\" wait times.\n",
    "\n",
    "2. **Model Interpretation**:\n",
    "   - The output of `model.summary().tables[1]` will display the coefficient for `C(kind)[T.long]`, which is the estimated difference between the mean eruption duration for the \"long\" wait time group and the \"short\" wait time group.\n",
    "   - The p-value for this coefficient will help you assess the evidence against the null hypothesis that there is no difference between the two groups.\n",
    "\n",
    "3. **Box Plot Visualization**:\n",
    "   - The box plot visualizes the distribution of eruption durations for both \"short\" and \"long\" wait times, which helps to visually assess whether there seems to be a difference in the distributions.\n",
    "\n",
    "### Reporting the Evidence:\n",
    "- **Null Hypothesis**: The null hypothesis is that the **mean duration** for \"short\" and \"long\" wait times is the same.\n",
    "- **Evidence Against the Null**: If the p-value for the `C(kind)[T.long]` coefficient is small (typically below 0.05), you can reject the null hypothesis and conclude that there is a significant difference between the two groups. If the p-value is larger, you would fail to reject the null hypothesis, suggesting no significant difference between the groups.\n",
    "\n",
    "By running the code above, you will be able to interpret the results and check whether there is a statistically significant difference between the two groups (\"short\" and \"long\") in terms of their eruption durations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694e7f5",
   "metadata": {},
   "source": [
    "# 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b66298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original observed statistic (difference in mean durations between \"short\" and \"long\")\n",
    "observed_diff = old_faithful.groupby('kind')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "\n",
    "# Initialize list to store the difference in means from shuffled datasets\n",
    "perm_diffs = []\n",
    "\n",
    "# Perform permutation testing (e.g., 10000 permutations)\n",
    "for _ in range(10000):\n",
    "    shuffled_data = old_faithful.assign(\n",
    "        kind_shuffled=old_faithful['kind'].sample(n=len(old_faithful), replace=False).values\n",
    "    )\n",
    "    # Calculate the mean difference between the shuffled \"short\" and \"long\" groups\n",
    "    perm_diff = shuffled_data.groupby('kind_shuffled')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "    perm_diffs.append(perm_diff)\n",
    "\n",
    "# Calculate p-value\n",
    "p_value_perm = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n",
    "print(f\"Permutation Test p-value: {p_value_perm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store bootstrap mean differences\n",
    "bootstrap_diffs = []\n",
    "\n",
    "# Perform bootstrap resampling (e.g., 10000 bootstrap samples)\n",
    "for _ in range(10000):\n",
    "    # Resample within each group (\"short\" and \"long\")\n",
    "    bootstrapped_data = old_faithful.groupby('kind').apply(\n",
    "        lambda x: x.sample(n=len(x), replace=True)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate the mean difference between \"short\" and \"long\" in the bootstrapped sample\n",
    "    bootstrap_diff = bootstrapped_data.groupby('kind')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "    bootstrap_diffs.append(bootstrap_diff)\n",
    "\n",
    "# Calculate 95% bootstrap confidence interval\n",
    "bootstrap_ci = np.quantile(bootstrap_diffs, [0.025, 0.975])\n",
    "print(f\"Bootstrap 95% Confidence Interval: {bootstrap_ci}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7bb244",
   "metadata": {},
   "source": [
    "(a) Explanation of Sampling Approaches:\n",
    "Permutation Test:\n",
    "\n",
    "The permutation test is a distribution-free test, meaning it does not rely on assumptions about the underlying distribution of the data (e.g., normality).\n",
    "It works by shuffling the group labels to simulate the null hypothesis scenario (no difference between groups). The test statistic is recalculated for each shuffled dataset, generating a distribution of test statistics under the null hypothesis.\n",
    "The observed difference is compared to this distribution to calculate a p-value, which tells us how likely it is to observe a difference as extreme as the one observed in the data if the null hypothesis were true.\n",
    "Bootstrap Confidence Interval:\n",
    "\n",
    "Bootstrapping works by resampling from the observed data with replacement to estimate the sampling distribution of the statistic of interest (in this case, the difference in means).\n",
    "By generating many resampled datasets, we can compute a confidence interval for the difference in means. This method is also distribution-free, as it relies on the observed data rather than assumptions about the underlying distribution.\n",
    "The 95% confidence interval gives the range of values within which we would expect the true difference in means to lie with 95% confidence.\n",
    "(b) Comparison and Contrast with Indicator Variable-Based Model Approach:\n",
    "Indicator Variable-Based Model (from Question 11):\n",
    "\n",
    "This approach uses an Ordinary Least Squares (OLS) regression model with the indicator variable (kind) as the predictor.\n",
    "The model assumes that the difference in mean duration between \"short\" and \"long\" groups can be explained by the indicator variable kind. This model directly estimates the difference between the groups and tests if the coefficient for kind is significantly different from zero (indicating a difference between the two groups).\n",
    "The p-value from the model summary tells us if the difference between groups is statistically significant.\n",
    "Permutation Test:\n",
    "\n",
    "The permutation test also compares the means of the two groups, but it does so by randomly shuffling the group labels and testing whether the observed difference is extreme relative to the distribution generated by this random shuffling.\n",
    "Unlike the indicator variable-based model, the permutation test does not assume any underlying distribution and directly tests the null hypothesis by generating a distribution of test statistics under the null.\n",
    "Bootstrap Confidence Interval:\n",
    "\n",
    "Bootstrapping is similar in concept to the indicator variable-based model because it estimates the difference in means, but it does so by generating a distribution of mean differences through repeated resampling.\n",
    "While the OLS model provides a single estimate of the difference and a p-value, the bootstrap method provides a range (confidence interval) for the difference in means, offering more information about the uncertainty of the estimate.\n",
    "Conclusion:\n",
    "Both the permutation test and bootstrap confidence interval methods are more flexible and do not assume any specific distribution of the data, unlike the indicator variable-based model, which relies on assumptions about the linearity and error structure of the data.\n",
    "The permutation test is focused on hypothesis testing (p-value), while the bootstrap method provides a confidence interval, giving a more direct estimate of the uncertainty in the difference between the groups.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e6c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59c724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4e33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74117e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db03aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46cc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a0ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d35b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
